# ConvLSTM: Many-to-One
from numpy.random import seed
seed(1)
from tensorflow import set_random_seed
set_random_seed(2)
from math import sqrt
from numpy import concatenate
from numpy import array
from matplotlib import pyplot
from pandas import read_csv
from pandas import DataFrame
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras import optimizers
from numpy import nan
from keras.layers.core import Dense, Activation, Dropout
from keras.layers.recurrent import LSTM
import time
from keras import layers
from keras.optimizers import RMSprop
from keras.optimizers import SGD
from keras.optimizers import Adam
from keras.callbacks import  Callback, TensorBoard, ModelCheckpoint, EarlyStopping
import matplotlib.pyplot as plt
from keras.layers import RepeatVector
from keras.layers import TimeDistributed, Activation
from keras.layers import LSTM, CuDNNLSTM
from keras.layers import ConvLSTM2D
from keras.layers import Flatten
from pandas import concat
from keras.models import load_model
from keras.layers import BatchNormalization
from keras.callbacks import  Callback, TensorBoard, ModelCheckpoint, EarlyStopping


def parser(x):
    
        return datetime.datetime.strptime(''+x,"%Y-%m-%d %H:%M:%S")

## load data
dataset = pd.read_csv('drive/Colab Notebooks/disneyland.csv', header=0, parse_dates=[0], index_col=[0])
print(dataset.shape)
print(dataset.head(5))
values = dataset.values
# specify columns to plot
groups = [0,1,2,3,4]
i = 1
# plot each column
pyplot.figure()
for group in groups:
    pyplot.subplot(len(groups), 1, i)
    pyplot.plot(values[:, group])
    pyplot.title(dataset.columns[group], y=0, loc='right')
    i += 1
pyplot.show()
dataset.drop('username', axis=1, inplace=True)
dataset.plot()
pyplot.show()

# get a list of columns
cols = list(dataset)
# move last column to the begining
cols.insert(0, cols.pop(cols.index('talking_about_count')))
dataset = dataset.loc[:, cols]
# Create new file
dataset.to_csv('tempDysland.csv')

# specify the number of lag hours
#n_test = 300
n_hours_past = 7
n_hours_future = 4
n_features = 4
#Initialize variables
validations, rmses, yhats, predictions, actuals, t = list(), list(), list(), list(), list(), list()
n_neurons = 300
n_batch = 357
n_epoch = 10000

# convert series to supervised learning
def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):
    n_vars = 1 if type(data) is list else data.shape[1]
    df = DataFrame(data)
    cols, names = list(), list()
    # input sequence (t-n, ... t-1)
    for i in range(n_in, 0, -1):
        cols.append(df.shift(i))
        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]
    # forecast sequence (t, t+1, ... t+n)
    for i in range(0, n_out):
        cols.append(df.shift(-i))
        if i == 0:
            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]
        else:
            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]
    # put it all together
    agg = concat(cols, axis=1)
    agg.columns = names
    # drop rows with NaN values
    if dropnan:
        agg.dropna(inplace=True)
    return agg
  
def get_input(train,  test):    
    n_obs = n_hours_past * 3
    train_X = train[:, :n_obs]
    test_X = test[:, :n_obs]
    # reshape input to be 3D [samples, timesteps, features]
    train_X = train_X.reshape((train_X.shape[0], n_hours_past, 3))
    test_X = test_X.reshape((test_X.shape[0], n_hours_past, 3))
    return train_X, test_X
  
def framed_out():  
    # Here we take n_hours_future columns of the following n_hours_future outage values
    names = list()
    n_vars = 4
    for i in range(0, n_hours_future):
        if i == 0:
            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]
            #names += ['var1(t)']
        else:
            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]
            #names += [('var1(t+%d)' % (i))]

    output = reframed[names]
    print(output.head())
    
    out_values = output.values
    #reshape output to be 2D [samples, timesteps]
    ##out_values = out_values.reshape((out_values.shape[0], n_hours_future))
    #reshape output to be 3D [samples, timesteps]
    out_values = out_values.reshape((out_values.shape[0], n_hours_future, n_features))
    print(out_values.shape)
    return out_values
  
def new_input(data, n_in=1, n_out=1, dropnan=True):
    #data = data[:,1:]
    n_vars = 1 if type(data) is list else data.shape[1]
    df = DataFrame(data)
    cols, variables = list(), list()
    # input sequence (t-n, ... t-1)
    for i in range(n_in, 0, -1):
        cols.append(df.shift(i))
        variables += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]
    # forecast sequence (t, t+1, ... t+n)
    for i in range(0, n_out):
        cols.append(df.shift(-i))
        if i == 0:
            variables += [('var%d(t)' % (j+1)) for j in range(n_vars)]
        else:
            variables += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]
    # put it all together
    agg = concat(cols, axis=1)
    agg.columns = variables
    # drop rows with NaN values
    if dropnan:
        agg.dropna(inplace=True)
    return agg
  
  
def fit_lstm(train_X, train_y, n_batch, n_epoch, n_neurons):
  model = Sequential()
  model.add(ConvLSTM2D(filters=64, kernel_size=(1,3), activation='relu',
  batch_input_shape=(n_batch,1, 1, 7, 3), padding='same', return_sequences = True, kernel_initializer='glorot_uniform' ))
  model.add(BatchNormalization())
  #model.add(Dropout(0.5))
  model.add(ConvLSTM2D(filters=64, kernel_size=(1,3), activation='relu', return_sequences = True, padding='same'))
  model.add(BatchNormalization())
  #model.add(Dropout(0.5))
  model.add(ConvLSTM2D(filters=64, kernel_size=(1,3), activation='relu', return_sequences = True, padding='same'))
  #model.add(Dropout(0.5))
  model.add(BatchNormalization())
  model.add(ConvLSTM2D(filters=64, kernel_size=(1,3), activation='relu', return_sequences = False, padding='same'))
  #model.add(Dropout(0.5))
  model.add(BatchNormalization())
  model.add(Flatten())
  model.add(RepeatVector(n_hours_future))
  model.add(CuDNNLSTM(n_neurons, batch_input_shape=(n_batch,train_X.shape[1], train_X.shape[2]), return_sequences = True, stateful = True))
  #model.add(CuDNNLSTM(200,return_sequences= True))
  model.add(Activation('relu'))
  model.add(BatchNormalization())
  #model.add(Dropout(0.5))
  model.add(CuDNNLSTM(n_neurons,return_sequences= True))
  model.add(Activation('relu'))
  model.add(BatchNormalization())
  #model.add(Dropout(0.5))
  model.add(CuDNNLSTM(n_neurons,return_sequences= True))
  model.add(Activation('relu'))
  model.add(BatchNormalization())
  #model.add(Dropout(0.5))
  model.add(CuDNNLSTM(n_neurons,return_sequences= True))
  model.add(Activation('relu'))
  model.add(BatchNormalization())
  #model.add(Dropout(0.5))
  model.add(CuDNNLSTM(n_neurons,return_sequences= True))
  model.add(Activation('relu'))
  model.add(Dropout(0.5))
  model.add(TimeDistributed(Dense(500, activation = 'linear')))
  model.add(TimeDistributed(Dense(n_hours_future)))
  #model.summary()
  ##print (model.output_shape)
  adam = Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)
  model.compile(loss='mean_squared_error',optimizer = adam)
  ##model.compile(loss='mean_squared_error', optimizer= 'adam')
  #Callback
  cb = EarlyStopping(monitor='loss',min_delta=0, patience=30,verbose = 0, mode='auto', baseline=None, restore_best_weights=True)
  # fit
  loss, val_Loss = list(), list()
  for i in range(n_epoch):
    history = model.fit(train_X,train_y,batch_size= n_batch,epochs = 1,
                        verbose=0,shuffle=False,#validation_split = 0.1, 
                        callbacks=[cb])
    eqm = history.history['loss']
    #eqm_val = history.history['val_loss']
    loss.append(eqm)
    #val_Loss.append(eqm_val)
    model.reset_states()
  pyplot.plot(loss, label='train')
  #pyplot.plot(val_Loss, label='test')
  pyplot.legend()
  pyplot.savefig('drive/Images/LearningCurveDysland.png', dpi=200)
  pyplot.show()
  #return model
  
  ##re-define the batch size
  n_batch = 1
  #re-define model
  new_model = Sequential()
  new_model.add(ConvLSTM2D(filters=64, kernel_size=(1,3), activation='relu',
  batch_input_shape=(n_batch,1, 1, 7, 3), padding='same', return_sequences = True, kernel_initializer='glorot_uniform' ))
  new_model.add(BatchNormalization())
  #model.add(Dropout(0.5))
  new_model.add(ConvLSTM2D(filters=64, kernel_size=(1,3), activation='relu', return_sequences = True, padding='same'))
  new_model.add(BatchNormalization())
  #model.add(Dropout(0.5))
  new_model.add(ConvLSTM2D(filters=64, kernel_size=(1,3), activation='relu', return_sequences = True, padding='same'))
  #model.add(Dropout(0.5))
  new_model.add(BatchNormalization())
  new_model.add(ConvLSTM2D(filters=64, kernel_size=(1,3), activation='relu', return_sequences = False, padding='same'))
  #model.add(Dropout(0.5))
  new_model.add(BatchNormalization())
  new_model.add(Flatten())
  new_model.add(RepeatVector(n_hours_future))
  new_model.add(CuDNNLSTM(n_neurons, batch_input_shape=(n_batch, train_X.shape[1], train_X.shape[2]), return_sequences = True, stateful = True))
  #model.add(CuDNNLSTM(200,return_sequences= True))
  new_model.add(Activation('relu'))
  new_model.add(BatchNormalization())
  #new_model.add(Dropout(0.5))
  new_model.add(CuDNNLSTM(n_neurons,return_sequences= True))
  new_model.add(Activation('relu'))
  new_model.add(BatchNormalization())
  #model.add(Dropout(0.5))
  new_model.add(CuDNNLSTM(n_neurons,return_sequences= True))
  new_model.add(Activation('relu'))
  #new_model.add(Dropout(0.5))
  new_model.add(BatchNormalization())
  new_model.add(CuDNNLSTM(n_neurons,return_sequences= True))
  new_model.add(Activation('relu'))
  new_model.add(BatchNormalization())
  #new_model.add(Dropout(0.5))
  new_model.add(CuDNNLSTM(n_neurons,return_sequences= True))
  new_model.add(Activation('relu'))
  new_model.add(Dropout(0.5))
  new_model.add(TimeDistributed(Dense(500, activation = 'linear')))
  new_model.add(TimeDistributed(Dense(n_hours_future)))
  #copy weights
  old_weights = model.get_weights()
  new_model.set_weights(old_weights)
  #optimizer = RMSprop(lr=0.01, rho=0.9, epsilon=None, decay=0.6)
  #new_model.compile(loss='mean_squared_error', optimizer=optimizer)
  adam = Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)
  new_model.compile(loss='mean_squared_error',optimizer = adam) 
  return new_model
    
# make a one-step forecast
def forecast_lstm(model, n_batch, X):
    #X = X.reshape((X.shape[0], X.shape[1], X.shape[2]))
    yhat = np.array(model.predict(X, n_batch))
    #yhat = np.array(model.predict(X, batch_size = n_batch))
    #return yhat[0,0]  
    return yhat  

def computeHCF(x, y):
    if x > y:
        smaller = y    
    else:
        smaller = x
    for i in range(1, smaller+1):
        if((x % i == 0) and (y % i == 0)):
            hcf = i

    return hcf
  

#Prepare input data
data = pd.read_csv('tempDysland.csv', parse_dates=[0], index_col=[0])
n_records = len(data)
n_train= int(n_records * 0.75)
n_test = n_records - n_train
#data.drop('Preci', axis = 1, inplace = True)
#data.drop('WindDir', axis = 1, inplace = True)
print(data.shape)
values = data.values
train_data, test_data = values[:n_train], values[-n_test:]

print(train_data.shape, test_data.shape)
# ensure all data is float
train_data = train_data.astype('float32')
test_data = test_data.astype('float32')
# normalize features
scaler = MinMaxScaler(feature_range=(0, 1))
scaled1 = scaler.fit_transform(train_data)
scaled2 = scaler.transform(test_data)

# frame as supervised learning: convert into input/output
scaled1 = DataFrame(scaled1)
scaled2 = DataFrame(scaled2)
scaled = scaled2.append(scaled1)
scaled = scaled.values
print(scaled.shape)
n_records2 = len(scaled)
reframed = series_to_supervised(scaled, n_hours_past, n_hours_future)

## ========================================Preapare Output Data ===========================

data = pd.read_csv('tempDysland.csv', parse_dates=[0], index_col=[0])
n_records = len(data)
n_train= int(n_records * 0.75)
n_test = n_records - n_train
print(data.shape)
values = data.values
train_data, test_data = values[:n_train], values[-n_test:]
print(train_data.shape, test_data.shape)
# ensure all data is float
train_data = train_data.astype('float32')
test_data = test_data.astype('float32')
# normalize features
scaler2 = MinMaxScaler(feature_range=(0, 1))
scaledt = scaler2.fit_transform(train_data)
scaledu = scaler2.transform(test_data)

# frame as supervised learning: convert into input/output
scaledt = DataFrame(scaledt)
scaledu = DataFrame(scaledu)
scalednew = scaledu.append(scaledt)
scalednew = scalednew.values
print(scalednew.shape)
n_records2 = len(scalednew)

reframed3 = new_input(scalednew, n_hours_past,n_hours_future)
#print(reframed3.head())

# split into train and test sets
values = reframed3.values
y = framed_out()

n_test = n_records - n_train
print("Total Samples:", n_records)
print("Minimum Training Samples:", n_train)
print("Test Samples:", n_test)

train, test = values[0:n_train], values[n_train:n_train + n_test]
train_X, test_X = get_input(train, test)
train_y, test_y = y[0:n_train], y[n_train:n_train + n_test]
#for i in range(5):
  #print(train_X[i],train_y[i] )

# reshape into subsequences [samples, time steps, rows, cols, channels]
train_X = train_X.reshape((train_X.shape[0], 1, 1, 7, 3))
# reshape into [samples, time steps, rows, cols, channels]
test_X = test_X.reshape((test_X.shape[0], 1, 1, 7, 3))
##train_y = train_y.reshape((train_y.shape[0], n_hours_future, n_features))
#test_y = test_y.reshape((test_y.shape[0], n_hours_future * n_features))
print(train_X.shape, train_y.shape)
print(test_X.shape, test_y.shape)

lstm_model = fit_lstm(train_X, train_y, n_batch, n_epoch, n_neurons)
n_batch =1
yhat = forecast_lstm(lstm_model, n_batch, test_X)        
test_X_reshaped = test_X.reshape((test_X.shape[0], n_hours_past*3))

yhat_i = yhat[:,0:4,0]
yhat_i = yhat_i.reshape(yhat_i.shape[0], -1)
print(yhat_i.shape)

test_y_i = test_y[:,0:4,0]
test_y_i = test_y_i.reshape(test_y_i.shape[0], -1)
print(test_y_i.shape)

# invert normalization of forecast values and actual values
for i in range(n_hours_future):
  inv_yhat_i = yhat_i[:,i]
  inv_yhat_i = inv_yhat_i.reshape(inv_yhat_i.shape[0], 1)
  inv_yhat_i = concatenate((inv_yhat_i, test_X_reshaped[:, -3:]), axis=1)
  inv_yhat_i = scaler.inverse_transform(inv_yhat_i)
  inv_yhat_i = inv_yhat_i[:,0]
      
  inv_y_i = test_y_i[:,i]
  inv_y_i = inv_y_i.reshape(inv_y_i.shape[0], 1)
  inv_y_i = concatenate((inv_y_i, test_X_reshaped[:, -3:]), axis=1)
  inv_y_i = scaler.inverse_transform(inv_y_i)
  inv_y_i = inv_y_i[:,0]
  
  rmse_i = sqrt(mean_squared_error(inv_yhat_i, inv_y_i))
  #mae_i = np.mean(np.absolute(inv_y_i - inv_yhat_i))
  std_i = np.std(np.absolute(inv_y_i - inv_yhat_i))
  print('Test RMSE and STD %d Day(s) Ahead: %.3f, %.3f' % (i+1,rmse_i,std_i))
print('%d Day(s) Ahead Forecast:' %(i+1) , inv_yhat_i[-(i+1):])
print('Test RMSE and STD: %.3f, %.3f' % (rmse_i,std_i))
    
inv_y_i = array(inv_y_i)
inv_yhat_i = array(inv_yhat_i)

a = np.hstack(inv_y_i)
b = np.hstack(inv_yhat_i)
print(a.shape)
print(b.shape)

a = DataFrame(a)
b = DataFrame(b)

# Plot Overall Graphs
indices, indices_data = list(), list()
indices= data.index.values
indices_data = data.index.values


import matplotlib.pyplot as plt
fig, ax = plt.subplots()
bb=[i for i in indices_data]
aa=[x for x in indices[-110:]]
print(len(actuals))
ax.plot(bb, data, label='Observed', color='#006699')
ax.plot(aa, a, label="Actual")
ax.plot(aa, b, 'r', label="Predicted")
fig.autofmt_xdate()
plt.ylabel('Talking-about', size=10)
plt.xlabel('Time Step(days)', size=10)
plt.legend(fontsize=10)
plt.legend(loc='upper left')
plt.title('All Data Distribution')
plt.show()

import matplotlib.pyplot as plt
fig, ax = plt.subplots()
ax.plot(aa, a, label="Actual")
ax.plot(aa, b, 'r', label="Predicted")
fig.autofmt_xdate()
plt.ylabel('Talking-about', size=10)
plt.xlabel('Time Step(days)', size=10)
plt.legend(fontsize=10)
plt.legend(loc='upper left')
plt.title('Actual Vs. Predicted')
pyplot.savefig('drive/Images/dyslandImgage.png', dpi=200)
plt.show()
